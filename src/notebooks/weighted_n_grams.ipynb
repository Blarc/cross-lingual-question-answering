{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Checking for translated answers in the translated context\n",
    "\n",
    "The purpose of this notebook is to check if the translation of the answer exists in the translated context.\n",
    "This is done by extracting N-grams, transforming them to word embeddings using fasttext model and\n",
    "calculating cosine similarity between them.\n",
    "\n",
    "The algorithm is described in an article called [Sentence Similarity Techniques for Short vs Variable Length Text using Word Embeddings](https://www.researchgate.net/publication/338283181_Sentence_Similarity_Techniques_for_Short_vs_Variable_Length_Text_using_Word_Embeddings)\n",
    "by *Dudekula, Shashavali & Vishwjeet, V. & Kumar, Rahul & Mathur, Gaurav & Nihal, Nikhil & Mukherjee, Siddhartha & Patil, Suresh (2019)\n",
    "Computación y Sistemas. 23. 10.13053/cys-23-3-3273*.\n",
    "\n",
    "\\\n",
    "<img src=\"../../docs/imgs/text_similarity_graph.PNG\" alt=\"drawing\" width=\"800\"/>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "model = fasttext.load_model('../../models/cc.sl.300.bin')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-grams: [['Francija'], ['Test'], ['123'], ['Haha'], ['Test']]\n",
      "2-grams: [['Francija', 'Test'], ['Test', '123'], ['123', 'Haha'], ['Haha', 'Test']]\n",
      "3-grams: [['Francija', 'Test', '123'], ['Test', '123', 'Haha'], ['123', 'Haha', 'Test']]\n",
      "4-grams: [['Francija', 'Test', '123', 'Haha'], ['Test', '123', 'Haha', 'Test']]\n",
      "5-grams: [['Francija', 'Test', '123', 'Haha', 'Test']]\n"
     ]
    }
   ],
   "source": [
    "from src.utils.translation_utils import get_grams\n",
    "\n",
    "test = 'Francija Test 123 Haha Test'\n",
    "for ix, g in enumerate(get_grams(test, 1, 100)):\n",
    "    print(f'{ix+1}-grams: {g}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Example of how function `find_similar_text` works."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: kot 1: poudarjata 2: fortnow 3: & 4: homer 5: 2003 6: začetek 7: sistematičnih 8: študij 9: računske 10: kompleksnosti 11: pripisujemo 12: temeljnemu 13: članku 14: o 15: računalniški 16: zapletenosti 17: algoritmov 18: ki 19: sta 20: ga 21: napisala 22: juris 23: hartmanis 24: in 25: richard 26: stearns 27: 1965 28: ki 29: sta 30: določila 31: definicije 32: časovne 33: in 34: prostorske 35: kompleksnosti 36: in 37: dokazala 38: hierarhične 39: izreke 40: leta 41: 1965 42: je 43: edmonds 44: definiral 45: dober 46: algoritem 47: kot 48: dober 49: algoritem 50: ki 51: je 52: omejen 53: s 54: polinomom 55: vhodne 56: velikosti Kateri papir se običajno šteje za zvonec, ki se uporablja v sistematičnih študijah računske kompleksnosti?\n",
      "o računalniški zapletenosti algoritmov\n",
      "([14, 14, 14], 1.0000000819563866)\n"
     ]
    }
   ],
   "source": [
    "from src.utils.translation_utils import clean_text, find_similar_text\n",
    "import json\n",
    "\n",
    "with open('../../data/dev-v2.0_SL.json', 'r', encoding='UTF-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "theme_index = 1\n",
    "paragraph_index = 44\n",
    "qas_index = 0\n",
    "\n",
    "clean_context = clean_text(data[theme_index]['paragraphs'][paragraph_index]['context'])\n",
    "for index, i in enumerate(get_grams(clean_context)[0]):\n",
    "    for word in i:\n",
    "        print(f'{index}: {word}', end=' ')\n",
    "\n",
    "\n",
    "qas_number = 0\n",
    "answer = clean_text(data[theme_index]['paragraphs'][paragraph_index]['qas'][qas_number]['answers'][0]['text'])\n",
    "question = data[theme_index]['paragraphs'][paragraph_index]['qas'][qas_number]['question']        \n",
    "print(question)\n",
    "print(answer)\n",
    "print(find_similar_text(answer, clean_context, model))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Even though the words may look very similar, cosine similarity between their embedding vectors might not be big. Here is an example:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.28973785]], dtype=float32)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "a = 'Sena'\n",
    "b = 'Sene'\n",
    "\n",
    "a_embed = model.get_sentence_vector(a)\n",
    "b_embed = model.get_sentence_vector(b)\n",
    "\n",
    "cosine_similarity([a_embed], [b_embed])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code fixes the answers and cleans the text of our dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████████████████▏                         | 255/442 [08:13<06:32,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting answer to question: Temperature nad 100 stopinj bodo običajno najdene v kakšni višini Piemonta?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 442/442 [14:12<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed answers: 20313\n",
      "Removed qas: 94523\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data_name = 'train'\n",
    "\n",
    "with open(f'../../data/{data_name}-v2.0_SL.json', 'r', encoding='UTF-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "removed_qas = 0\n",
    "removed_answers = 0\n",
    "for i_theme, theme in enumerate(tqdm(data, ncols=100)):\n",
    "# for i_theme, theme in enumerate(data):\n",
    "    for i_paragraph, paragraph in enumerate(theme['paragraphs']):\n",
    "        context = paragraph['context']\n",
    "        clean_context = clean_text(context)  \n",
    "        data[i_theme]['paragraphs'][i_paragraph]['context'] = clean_context\n",
    "\n",
    "        for i_qas, qas in enumerate(paragraph['qas']):\n",
    "            question = qas['question']\n",
    "            \n",
    "            for i_answer, answer in enumerate(qas['answers']):\n",
    "                answer_text = answer['text']\n",
    "                answer_text = clean_text(answer_text).rstrip().lstrip()\n",
    "                \n",
    "                if len(answer_text) == 0:\n",
    "                    print(f'Deleting answer to question: {question}')\n",
    "                    qas['answers'].remove(answer)\n",
    "                    continue\n",
    "                \n",
    "                answer_start = int(answer['answer_start'])\n",
    "                answer_split = answer_text.split(' ')\n",
    "                \n",
    "                if answer_start < len(clean_context):\n",
    "                    reduced_context_start = max(answer_start - 200, 0)\n",
    "                    reduced_context_clean = clean_context[max(answer_start - 200, 0): min(answer_start + len(answer_text) + 200, len(clean_context))]\n",
    "                else:\n",
    "                    reduced_context_start = 0\n",
    "                    reduced_context_clean = clean_context\n",
    "                \n",
    "                reduced_context_clean.lstrip().rstrip()\n",
    "                reduced_context_clean_split = reduced_context_clean.split(' ')\n",
    "                context_split = clean_context.split(' ')\n",
    "\n",
    "                answer_length = len(answer_split)\n",
    "                start_indexes, avg_similarities = find_similar_text(answer_text, reduced_context_clean, model, answer_length, answer_length)\n",
    "                \n",
    "                if avg_similarities > 0.75:\n",
    "                    start_index = start_indexes[0]\n",
    "                    fixed_answer = ' '.join(reduced_context_clean_split[start_index:start_index + answer_length])\n",
    "                    fixed_index = reduced_context_start + reduced_context_clean.index(fixed_answer)\n",
    "                    data[i_theme]['paragraphs'][i_paragraph]['qas'][i_qas]['answers'][i_answer]['text'] = fixed_answer\n",
    "                    data[i_theme]['paragraphs'][i_paragraph]['qas'][i_qas]['answers'][i_answer]['answer_start'] = fixed_index\n",
    "                else:\n",
    "                    qas['answers'].remove(answer)\n",
    "                    removed_answers += 1\n",
    "                \n",
    "                    # print(f'{\"-\" * 100}')\n",
    "                    # print(f'CONTEXT: {clean_context}')\n",
    "                    # print(f'QUESTION: {question}')\n",
    "                    # print(f'BEFORE: {answer_text}')\n",
    "                    # print(f'AFTER: {fixed_answer}')\n",
    "                    # print(f'SIMILARITY: {avg_similarities}')\n",
    "                \n",
    "            # if 'plausible_answers' in qas:\n",
    "            #     for plausible_answer in qas['plausible_answers']:\n",
    "            #         plausible_answer = plausible_answer['text']\n",
    "        \n",
    "            if len(qas['answers']) == 0:\n",
    "                paragraph['qas'].remove(qas)\n",
    "            removed_qas += 1\n",
    "\n",
    "\n",
    "print(f'Removed answers: {removed_answers}')            \n",
    "print(f'Removed qas: {removed_qas}')            \n",
    "with open(f'../../data/{data_name}-v2.0_SL_fixed_removed.json', 'w', encoding='UTF-8') as file:\n",
    "    json.dump(data, file, sort_keys=True, indent=4, ensure_ascii=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}